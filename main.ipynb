{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7402218,"sourceType":"datasetVersion","datasetId":4304373}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nfrom transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch, torchaudio, torchtext\nimport torch.nn as nn\nimport os\nimport gc\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:11:27.617149Z","iopub.execute_input":"2024-01-23T12:11:27.617492Z","iopub.status.idle":"2024-01-23T12:11:43.599769Z","shell.execute_reply.started":"2024-01-23T12:11:27.617465Z","shell.execute_reply":"2024-01-23T12:11:43.598803Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    df_path = '/kaggle/input/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = '/kaggle/input/MM-USElecDeb60to16/audio_clips'\n    save_path = '/kaggle/working/'\n    df = pd.read_csv(df_path, index_col=0)\nexcept FileNotFoundError:\n    df_path = 'multimodal-dataset/files/MM-USElecDeb60to16/MM-USElecDeb60to16.csv'\n    audio_path = 'multimodal-dataset/files/MM-USElecDeb60to16/audio_clips'\n    save_path = 'multimodal-dataset/files'\n    df = pd.read_csv(df_path, index_col=0)\n# drop rows where audio length is 0\ndf = df[df['NewBegin'] != df['NewEnd']]\n\ntrain_df_complete = df[df['Set'] == 'TRAIN']\nval_df_complete = df[df['Set'] == 'VALIDATION']\ntest_df_complete = df[df['Set'] == 'TEST']\n\nDATASET_RATIO = 0.40\n# NUM_SAMPLES = 3000\n# indexes = np.random.choice(list(range(len(train_df_complete))), NUM_SAMPLES, replace=False)\n# train_idx = indexes[:int(0.7*NUM_SAMPLES)]\n# val_idx = indexes[int(0.7*NUM_SAMPLES):int(0.8*NUM_SAMPLES)]\n# test_idx = indexes[int(0.8*NUM_SAMPLES):]\n\ntrain_df = train_df_complete.iloc[:int(DATASET_RATIO * len(train_df_complete))]\nval_df = val_df_complete.iloc[:int(DATASET_RATIO * len(val_df_complete))]\ntest_df = test_df_complete.iloc[:int(DATASET_RATIO * len(test_df_complete))]","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:11:47.785633Z","iopub.execute_input":"2024-01-23T12:11:47.785993Z","iopub.status.idle":"2024-01-23T12:11:48.001802Z","shell.execute_reply.started":"2024-01-23T12:11:47.785965Z","shell.execute_reply":"2024-01-23T12:11:48.001019Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:11:50.140707Z","iopub.execute_input":"2024-01-23T12:11:50.141079Z","iopub.status.idle":"2024-01-23T12:11:50.168538Z","shell.execute_reply.started":"2024-01-23T12:11:50.141050Z","shell.execute_reply":"2024-01-23T12:11:50.167767Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                Text  Part Document  Order  \\\n0  CHENEY: Gwen, I want to thank you, and I want ...     1  30_2004      0   \n1  It's a very important event, and they've done ...     1  30_2004      1   \n2  It's important to look at all of our developme...     1  30_2004      2   \n3  And, after 9/11, it became clear that we had t...     1  30_2004      3   \n4  And we also then finally had to stand up democ...     1  30_2004      4   \n\n   Sentence  Start   End  Annotator                                   Tag  \\\n0         0   2101  2221        NaN                             {\"O\": 27}   \n1         1   2221  2304        NaN                             {\"O\": 19}   \n2         2   2304  2418        NaN                             {\"O\": 23}   \n3         3   2418  2744        NaN                {\"O\": 16, \"Claim\": 50}   \n4         4   2744  2974        NaN  {\"O\": 4, \"Claim\": 13, \"Premise\": 25}   \n\n  Component  ... Speaker SpeakerType    Set         Date  Year  \\\n0         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n1         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n2         O  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n3     Claim  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n4   Premise  ...  CHENEY   Candidate  TRAIN  05 Oct 2004  2004   \n\n                      Name MainTag NewBegin  NewEnd  idClip  \n0  Richard(Dick) B. Cheney       O   126.52  131.08  clip_0  \n1  Richard(Dick) B. Cheney       O   131.08  134.40  clip_1  \n2  Richard(Dick) B. Cheney       O   134.40  140.56  clip_2  \n3  Richard(Dick) B. Cheney   Claim   140.56  158.92  clip_3  \n4  Richard(Dick) B. Cheney   Mixed   158.92  172.92  clip_4  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Part</th>\n      <th>Document</th>\n      <th>Order</th>\n      <th>Sentence</th>\n      <th>Start</th>\n      <th>End</th>\n      <th>Annotator</th>\n      <th>Tag</th>\n      <th>Component</th>\n      <th>...</th>\n      <th>Speaker</th>\n      <th>SpeakerType</th>\n      <th>Set</th>\n      <th>Date</th>\n      <th>Year</th>\n      <th>Name</th>\n      <th>MainTag</th>\n      <th>NewBegin</th>\n      <th>NewEnd</th>\n      <th>idClip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHENEY: Gwen, I want to thank you, and I want ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2101</td>\n      <td>2221</td>\n      <td>NaN</td>\n      <td>{\"O\": 27}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>126.52</td>\n      <td>131.08</td>\n      <td>clip_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It's a very important event, and they've done ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2221</td>\n      <td>2304</td>\n      <td>NaN</td>\n      <td>{\"O\": 19}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>131.08</td>\n      <td>134.40</td>\n      <td>clip_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It's important to look at all of our developme...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2304</td>\n      <td>2418</td>\n      <td>NaN</td>\n      <td>{\"O\": 23}</td>\n      <td>O</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>O</td>\n      <td>134.40</td>\n      <td>140.56</td>\n      <td>clip_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>And, after 9/11, it became clear that we had t...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2418</td>\n      <td>2744</td>\n      <td>NaN</td>\n      <td>{\"O\": 16, \"Claim\": 50}</td>\n      <td>Claim</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Claim</td>\n      <td>140.56</td>\n      <td>158.92</td>\n      <td>clip_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And we also then finally had to stand up democ...</td>\n      <td>1</td>\n      <td>30_2004</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2744</td>\n      <td>2974</td>\n      <td>NaN</td>\n      <td>{\"O\": 4, \"Claim\": 13, \"Premise\": 25}</td>\n      <td>Premise</td>\n      <td>...</td>\n      <td>CHENEY</td>\n      <td>Candidate</td>\n      <td>TRAIN</td>\n      <td>05 Oct 2004</td>\n      <td>2004</td>\n      <td>Richard(Dick) B. Cheney</td>\n      <td>Mixed</td>\n      <td>158.92</td>\n      <td>172.92</td>\n      <td>clip_4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(train_df), len(test_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:11:52.286774Z","iopub.execute_input":"2024-01-23T12:11:52.287120Z","iopub.status.idle":"2024-01-23T12:11:52.293432Z","shell.execute_reply.started":"2024-01-23T12:11:52.287095Z","shell.execute_reply":"2024-01-23T12:11:52.292467Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(4967, 2986, 2758)"},"metadata":{}}]},{"cell_type":"code","source":"class BestModel:\n    \"\"\"\n        Class to keep track of the best performing model on validation set during training\n    \"\"\"\n    def __init__(self):\n        self.best_validation_loss = float('Infinity')\n        self.best_state_dict = None\n    def __call__(self, model, loss):\n        if loss < self.best_validation_loss:\n            self.best_validation_loss = loss\n            self.best_state_dict = model.state_dict()\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\"):\n    best_model_tracker = BestModel()\n    for epoch in tqdm(range(epochs)):\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n\n        for batch in train_loader:\n            optimizer.zero_grad()\n            #text_features, text_attention, audio_features, audio_attention, targets = batch\n            texts, audio_features, audio_attention, targets = batch\n            #text_features = text_features.to(device)\n            #text_attention = text_attention.to(device)\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            #output = model(text_features,text_attention,audio_features,audio_attention)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.detach()\n        training_loss = training_loss.cpu().item()\n        training_loss /= len(train_loader.dataset)\n\n        model.eval()\n        num_correct = 0 \n        num_examples = 0\n        for batch in val_loader:\n            #text_features, text_attention, audio_features, audio_attention, targets = batch\n            texts, audio_features, audio_attention, targets = batch\n            #text_features = text_features.to(device)\n            #text_attention = text_attention.to(device)\n            audio_features = audio_features.to(device)\n            audio_attention = audio_attention.to(device)\n            targets = targets.to(device)\n            #output = model(text_features,text_attention,audio_features,audio_attention)\n            output = model(texts,audio_features,audio_attention)\n            loss = loss_fn(output, targets)\n            valid_loss += loss.detach()\n            predicted_labels = torch.argmax(output, dim=-1)\n            correct = torch.eq(predicted_labels, targets).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        best_model_tracker(model, valid_loss)\n        valid_loss = valid_loss.cpu().item()\n        valid_loss /= len(val_loader.dataset)\n        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss, valid_loss, num_correct/num_examples))\n    model.load_state_dict(best_model_tracker.best_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:12:53.421078Z","iopub.execute_input":"2024-01-23T12:12:53.421454Z","iopub.status.idle":"2024-01-23T12:12:53.436612Z","shell.execute_reply.started":"2024-01-23T12:12:53.421424Z","shell.execute_reply":"2024-01-23T12:12:53.435525Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"text_model_card = 'bert-base-uncased'\naudio_model_card = 'facebook/wav2vec2-base-960h'\n\ntokenizer = BertTokenizer.from_pretrained(text_model_card)\nembedder = BertModel.from_pretrained(text_model_card).to(device)\n\nfor params in embedder.parameters():\n    params.requires_grad = False\n\nlabel_2_id = {\n    'Claim': 0,\n    'Premise': 1,\n    'O': 2\n}\n\nDOWNSAMPLE_FACTOR = 1/5\n\nclass MM_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, audio_dir, sample_rate):\n        self.audio_dir = audio_dir\n        self.sample_rate = sample_rate\n\n        self.audio_processor = AutoProcessor.from_pretrained(audio_model_card)\n        self.audio_model = AutoModel.from_pretrained(audio_model_card).to(device)\n        \n        # self.text_tokenizer = BertTokenizer.from_pretrained(text_model_card)\n        # self.text_model = BertModel.from_pretrained(text_model_card).to(device)\n        self.dataset = []\n\n        # Iterate over df\n        for _, row in tqdm(df.iterrows()):\n            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n            if os.path.exists(path):\n                # obtain audio WAV2VEC features\n                audio, sampling_rate = torchaudio.load(path)\n                if sampling_rate != self.sample_rate:\n                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n                    audio = torch.mean(audio, dim=0, keepdim=True)\n                with torch.inference_mode():\n                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n                    input_values = torch.tensor(input_values).to(device)\n                    audio_model_output = self.audio_model(input_values)\n                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n#                     audio_features = torch.nn.AvgPool1d(kernel_size=DOWNSAMPLE_FACTOR,stride=DOWNSAMPLE_FACTOR,)\n                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n                    audio_features = audio_features.permute(0,2,1)[0]\n                    audio_features = audio_features.cpu()\n                # obtain text BERT features\n                text = row['Text']\n\n                # token_text = self.text_tokenizer(text, return_tensors='pt').to(device)\n                # with torch.inference_mode():\n                #     text_features = self.text_model(**token_text)\n                #     text_features = text_features\n                #     text_features = text_features.cpu()\n                # del token_text\n                # gc.collect()\n\n                self.dataset.append((text, audio_features, label_2_id[row['Component']]))\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        return self.dataset[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:12:57.236322Z","iopub.execute_input":"2024-01-23T12:12:57.236683Z","iopub.status.idle":"2024-01-23T12:13:00.923891Z","shell.execute_reply.started":"2024-01-23T12:12:57.236654Z","shell.execute_reply":"2024-01-23T12:13:00.922875Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79997c07a94e47e8be17d01f7efb8e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0d0c3e591c44089d0fb45349e60bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456b77f73d2045d38db3c7a46297cdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2267fadc90f489588ae70c66a2c6f0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6d10bbfc2747539ce675a43a269f63"}},"metadata":{}}]},{"cell_type":"code","source":"try:\n    train_dataset = torch.load(f'{save_path}/train_dataset.pkl')\n    test_dataset = torch.load(f'{save_path}/test_dataset.pkl')\n    val_dataset = torch.load(f'{save_path}/val_dataset.pkl')\n    print('Restored datasets from memory')\nexcept:\n    print('Creating new datasets')\n    train_dataset = MM_Dataset(train_df, audio_path, 16_000)\n    test_dataset = MM_Dataset(test_df, audio_path, 16_000)\n    val_dataset = MM_Dataset(val_df, audio_path, 16_000)\n    torch.save(train_dataset, f'{save_path}/train_dataset.pkl')\n    torch.save(test_dataset, f'{save_path}/test_dataset.pkl')\n    torch.save(val_dataset, f'{save_path}/val_dataset.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:13:06.075461Z","iopub.execute_input":"2024-01-23T12:13:06.076270Z","iopub.status.idle":"2024-01-23T12:31:18.705026Z","shell.execute_reply.started":"2024-01-23T12:13:06.076239Z","shell.execute_reply":"2024-01-23T12:31:18.703580Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Creating new datasets\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75f7ac226dd490caa407f68d4ffed10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172048cedaeb4840a21e77722681c38c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e682535f2d4548b093231fe9fef73c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6926cb5de65943dfbc51e6022493d26c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d460303d5168465692ec9876ffba6461"}},"metadata":{}},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e500daf2c5c547cb87b74a511a0d8d34"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n4967it [08:13, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2986it [04:22, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ignored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\nIgnored unknown kwarg option normalize\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2758it [05:11,  8.84it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_dataloader(dataset, batch_size):\n    def pack_fn(batch):\n        texts = [x[0] for x in batch]\n        audio_features = [x[1] for x in batch]\n        labels = torch.tensor([x[2] for x in batch])\n\n        # tokenize text\n        # tokenizer_output = tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        # embed text\n        # embedder_output = embedder(**tokenizer_output, output_hidden_states=True)\n\n        # text_features = embedder_output['hidden_states'][0]\n        \n        # pad audio features\n        audio_features = pad_sequence(audio_features, batch_first=True, padding_value=float('-inf'))\n\n        audio_features_attention_mask = audio_features[:, :, 0] != float('-inf')\n        \n        audio_features[(audio_features == float('-inf'))] = 0\n\n        #return text_features, tokenizer_output.attention_mask, audio_features, audio_features_attention_mask, labels\n        return texts, audio_features, audio_features_attention_mask, labels\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pack_fn)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:31:18.707599Z","iopub.execute_input":"2024-01-23T12:31:18.708239Z","iopub.status.idle":"2024-01-23T12:31:18.718118Z","shell.execute_reply.started":"2024-01-23T12:31:18.708198Z","shell.execute_reply":"2024-01-23T12:31:18.716970Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataloader = create_dataloader(train_dataset, 8)\nval_dataloader = create_dataloader(val_dataset, 8)\ntest_dataloader = create_dataloader(test_dataset, 8)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:31:18.720526Z","iopub.execute_input":"2024-01-23T12:31:18.721121Z","iopub.status.idle":"2024-01-23T12:31:19.382175Z","shell.execute_reply.started":"2024-01-23T12:31:18.721080Z","shell.execute_reply":"2024-01-23T12:31:19.381001Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#del early_fusion\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:31:19.384907Z","iopub.execute_input":"2024-01-23T12:31:19.385303Z","iopub.status.idle":"2024-01-23T12:31:20.447938Z","shell.execute_reply.started":"2024-01-23T12:31:19.385267Z","shell.execute_reply":"2024-01-23T12:31:20.447025Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"48"},"metadata":{}}]},{"cell_type":"code","source":"class EarlyFusion(nn.Module):\n    def __init__(self, tokenizer, embedder, transformer, head):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.transformer = transformer\n        self.head = head\n\n    #def forward(self, text_features, text_attentions, audio_features, audio_attentions):\n    def forward(self, texts, audio_features, audio_attentions):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['hidden_states'][0]\n        text_attentions = tokenizer_output.attention_mask\n        \n        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n        concatenated_attentions = torch.cat((text_attentions, audio_attentions.float()), dim=1)\n        \n        # padding mask is 1 where there is padding (i.e. where attention is 0) and 0 otherwise\n        concatenated_padding_mask = ~concatenated_attentions.to(torch.bool)\n        \n        # compute a full attention mask of size [seq_len, seq_len]\n        full_attention_mask = torch.zeros((concatenated_features.shape[1], concatenated_features.shape[1]), dtype=torch.bool).to(device)\n                \n        transformer_output = self.transformer(src=concatenated_features,  mask=full_attention_mask, src_key_padding_mask=concatenated_padding_mask)\n        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)\n\ntransformer_layer = nn.TransformerEncoderLayer(d_model=768, nhead=4, dim_feedforward=512, batch_first=True).to(device)\ntransformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4).to(device)\n\nhead = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\nearly_fusion = EarlyFusion(tokenizer, embedder, transformer_encoder, head).to(device)\n\noptimizer = torch.optim.Adam(early_fusion.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\ntrain(early_fusion, optimizer, criterion, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:32:43.682629Z","iopub.execute_input":"2024-01-23T12:32:43.682991Z","iopub.status.idle":"2024-01-23T12:46:30.924949Z","shell.execute_reply.started":"2024-01-23T12:32:43.682954Z","shell.execute_reply":"2024-01-23T12:46:30.924016Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [01:22<12:26, 82.94s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.12, Validation Loss: 0.12, accuracy = 0.53\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [02:45<11:02, 82.82s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.11, Validation Loss: 0.12, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [04:08<09:39, 82.73s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.10, Validation Loss: 0.13, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [05:30<08:16, 82.71s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.09, Validation Loss: 0.14, accuracy = 0.53\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [06:53<06:53, 82.70s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.09, Validation Loss: 0.16, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [08:16<05:30, 82.70s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.08, Validation Loss: 0.16, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [09:39<04:08, 82.71s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.07, Validation Loss: 0.17, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [11:01<02:45, 82.68s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.07, Validation Loss: 0.17, accuracy = 0.55\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [12:24<01:22, 82.68s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.06, Validation Loss: 0.19, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [13:47<00:00, 82.72s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.06, Validation Loss: 0.18, accuracy = 0.54\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class TextModel(nn.Module):\n    def __init__(self, tokenizer, embedder, head):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.head = head\n    def forward(self, texts):\n        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n        text_features = embedder_output['last_hidden_state']\n        \n        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n        return self.head(text_features_pooled)\n    \nclass AudioModel(nn.Module):        \n    def __init__(self, transformer, head):\n        super().__init__()\n        self.transformer = transformer\n        self.head = head\n        \n    def forward(self, audio_features, audio_attention):\n        padding_mask = ~audio_attention.to(torch.bool)\n        full_attention_mask = torch.zeros((audio_features.shape[1],audio_features.shape[1]), dtype=torch.bool).to(device)\n        transformer_output = self.transformer(src=audio_features, mask=full_attention_mask, src_key_padding_mask=padding_mask)\n        \n        # pooling transformer output\n        transformer_output_sum = (transformer_output * audio_attention.unsqueeze(-1)).sum(axis=1)\n        transformer_output_pooled = transformer_output_sum / audio_attention.sum(axis=1).unsqueeze(-1)\n        return self.head(transformer_output_pooled)\n    \nclass EnsemblingFusion(nn.Module):\n    def __init__(self, text_model, audio_model):\n        super().__init__()\n        self.text_model = text_model\n        self.audio_model = audio_model\n        self.weight = torch.nn.Parameter(torch.tensor(0.0))\n        \n    def forward(self, texts, audio_features, audio_attentions):\n        text_logits = self.text_model(texts)\n        audio_logits = self.audio_model(audio_features, audio_attentions)\n        \n        text_probabilities = torch.nn.functional.softmax(text_logits)\n        audio_probabilities = torch.nn.functional.softmax(audio_logits)\n        \n        coefficient = (torch.tanh(self.weight) + 1) / 2\n        \n        return coefficient*text_probabilities + (1-coefficient)*audio_probabilities\n    \ntext_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\naudio_head = nn.Sequential(\n    nn.Linear(768, 256),\n    nn.ReLU(),\n    nn.Linear(256, 3)\n).to(device)\n\ntransformer_layer = nn.TransformerEncoderLayer(d_model=768, nhead=4, dim_feedforward=512, batch_first=True).to(device)\ntransformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4).to(device)\n\ntext_model = TextModel(tokenizer, embedder, text_head)\naudio_model = AudioModel(transformer_encoder, audio_head)\n\nensembling_fusion = EnsemblingFusion(text_model, audio_model)\n\noptimizer = torch.optim.Adam(early_fusion.parameters(), lr=1e-4)\n\ndef custom_loss(outputs, targets):\n    return torch.nn.functional.nll_loss(torch.log(outputs), targets, reduction='mean')\n\ntrain(ensembling_fusion, optimizer, custom_loss, train_dataloader, val_dataloader, epochs=10, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T12:06:40.488869Z","iopub.execute_input":"2024-01-23T12:06:40.489591Z","iopub.status.idle":"2024-01-23T12:08:05.352678Z","shell.execute_reply.started":"2024-01-23T12:06:40.489556Z","shell.execute_reply":"2024-01-23T12:08:05.351412Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [00:23<03:35, 23.99s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Training Loss: 0.14, Validation Loss: 0.14, accuracy = 0.35\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [00:47<03:11, 23.99s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.14, Validation Loss: 0.14, accuracy = 0.35\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [01:11<02:47, 23.96s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.14, Validation Loss: 0.14, accuracy = 0.35\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [01:24<03:17, 28.23s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(outputs, targets):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnll_loss(torch\u001b[38;5;241m.\u001b[39mlog(outputs), targets, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensembling_fusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[37], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m texts, audio_features, audio_attention, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#text_features = text_features.to(device)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#text_attention = text_attention.to(device)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m \u001b[43maudio_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m audio_attention \u001b[38;5;241m=\u001b[39m audio_attention\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}